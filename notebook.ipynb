{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.appconfig import FIRECRAWL_API_KEY\n",
    "\n",
    "import os\n",
    "from firecrawl import FirecrawlApp\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteScraper:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.firecrawl_api_key = FIRECRAWL_API_KEY\n",
    "        self.app = FirecrawlApp(api_key=self.firecrawl_api_key)\n",
    "        self.schema_fields = [{\"name\": \"\", \"type\": \"str\"}]\n",
    "\n",
    "    def create_dynamic_model(self, fields):\n",
    "        \"\"\"Create a dynamic Pydantic model from schema fields.\"\"\"\n",
    "        field_annotations = {}\n",
    "        for field in fields:\n",
    "            if field[\"name\"]:\n",
    "                type_mapping = {\n",
    "                    \"str\": str,\n",
    "                    \"bool\": bool,\n",
    "                    \"int\": int,\n",
    "                    \"float\": float\n",
    "                }\n",
    "                field_annotations[field[\"name\"]] = type_mapping[field[\"type\"]]\n",
    "        \n",
    "        return type(\n",
    "            \"ExtractSchema\",\n",
    "            (BaseModel,),\n",
    "            {\n",
    "                \"__annotations__\": field_annotations\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def create_schema_from_fields(self, fields):\n",
    "        \"\"\"Create schema using Pydantic model.\"\"\"\n",
    "        if not any(field[\"name\"] for field in fields):\n",
    "            return None\n",
    "        \n",
    "        model_class = self.create_dynamic_model(fields)\n",
    "        return model_class.model_json_schema()\n",
    "\n",
    "    def convert_to_table(self, data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert data to a pandas DataFrame and return as string.\"\"\"\n",
    "        if not data or 'data' not in data:\n",
    "            return \"\"\n",
    "        \n",
    "        df = pd.DataFrame([data['data']])\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    def scrape_website(self, website_url: str, prompt: str, schema_fields=None):\n",
    "        \"\"\"Main function to scrape website data.\"\"\"\n",
    "        if not website_url:\n",
    "            raise ValueError(\"Please provide a website URL\")\n",
    "\n",
    "        try:\n",
    "            schema = self.create_schema_from_fields(schema_fields) if schema_fields else None\n",
    "            \n",
    "            extract_params = {'prompt': prompt}\n",
    "            if schema:\n",
    "                extract_params['schema'] = schema\n",
    "\n",
    "            data = self.app.extract([website_url,],\n",
    "                                    extract_params\n",
    "                                    )\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "{'success': True, 'data': {'projects': [{'link': 'https://huggingface.co/spaces/raqibcodes/ai_voice_translator', 'title': 'AI Voice Translator', 'details': 'Record your message in English, and receive translations and transcriptions in multiple languages including Russian, French, Korean, Chinese, Spanish, Arabic and Japanese.'}, {'link': 'https://pdfgpt1.streamlit.app/', 'title': 'RAG Chatbot', 'details': 'Developed a Retrieval-Augmented Generation (RAG) chatbot with memory capabilities, supporting diverse document types. Integrated the Chroma vector store for efficient embedding storage and retrieval, optimized responses through advanced prompt engineering and LLM experimentation, and structured a modular, scalable, and collaborative codebase.'}, {'link': 'https://whatsappchats.streamlit.app/', 'title': 'AI-Powered WhatsApp Chat Analysis App', 'details': 'Developed and deployed an AI-powered WhatsApp chat analysis app with a chatbot that processes CSV chat files, enabling users to uncover insights effortlessly. The app integrates advanced NLP algorithms, including emoji analysis and keyword search, to extract meaningful insights from chat data. It features an interactive interface with dynamic visualizations, customizable charts, word clouds, and other plots, enhancing data exploration and user engagement.'}, {'link': 'https://predictsentiment.streamlit.app/', 'title': 'AI-Powered Sentiment Analysis Web App', 'details': 'Developed and deployed an AI-powered Sentiment Analysis web app with integrated Generative AI, allowing users to chat and derive insights from student feedback to enhance learning experiences. The app achieved 96% model accuracy, offering a deeper understanding of student emotions and assisting lecturers in refining teaching strategies. It also features EDA and interactive visualizations, improving data exploration and user engagement.'}, {'link': 'https://booksrecommender.streamlit.app/', 'title': 'Book Recommender System', 'details': \"This web-app is a book recommender system that uses a content-based approach, leveraging book features to recommend similar books to users based on features of the books or characteristics of the items and the user's preferences. Aspects such as plot, characters, and themes that truly engages the user are taken into consideration. Subsequently, book recommendations are provided that align with the user's tastes.\"}, {'link': 'https://github.com/Abdulraqib20/telecom-churn-prediction/tree/main/Notebooks', 'title': 'Telecom Churn Prediction', 'details': \"Addressing a significant industry challenge, this project effectively predicts customer churn in the telecom sector using machine learning. The top-performing model, an XGBoost classifier, achieved an impressive 92% Recall Score in predicting positive cases of 'churn' with other impressive performances in other evaluation metrics used too. Insights from this initiative empower the industry to implement strategic retention measures, mitigating churn and boosting customer satisfaction in a sector grappling with multimillion-dollar losses each month due to customer attrition.\"}, {'link': 'https://github.com/Abdulraqib20/Customer-Personality-Analysis/blob/main/customer_personality_analysis.ipynb', 'title': 'Customer Personality Analysis', 'details': 'This project allows businesses to customize their offerings, products and services to different customer segments, instead of utilizing a generic approach. Through techniques like unsupervised learning, I gain a better understanding of customers, facilitating adjustments to products based on specific needs, behaviors, and concerns of various customer types.'}, {'link': 'https://github.com/Abdulraqib20/Employee-Attrition-Prediction-Using-Gradient-Boosting-Algorithms', 'title': 'Employee Attrition Prediction', 'details': 'This project harnesses the power of data and AI to forecast and analyze employee attribution trends. Gradient Boosting Algorithms including XGBoost, CatBoost, LightGBM and an ensemble of these models were used to develop robust predictive models that provide insights into employee retention, identify patterns, and contribute to strategic talent management.'}, {'link': 'https://github.com/Abdulraqib20/Wrangle-and-Analyze-Data/blob/main/wrangle_act.ipynb', 'title': 'WeRateDogs Twitter Data Analysis', 'details': \"I wrangled, analyzed and visualized the tweet archive of Twitter user @dogrates also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. I wrangled WeRateDogs Twitter data to create interesting and trustworthy analysis and visualizations.\"}, {'link': 'https://github.com/Abdulraqib20/Prosper-Loan-Analysis/blob/main/Part_I_exploration_template.ipynb', 'title': 'Prosper Loan Analysis', 'details': 'In this project, I analyze the Prosper loan dataset using Python to create insightful visualizations. The process involves thorough exploration, data cleaning, and preparation to extract meaningful insights into borrower behavior and loan performance.'}, {'link': 'https://github.com/Abdulraqib20/Soccer-Database-Analysis/blob/main/project.ipynb', 'title': 'Soccer Database Analysis', 'details': 'This project was part of my Data Analyst Nanodegree Programme at Udacity. The soccer database, sourced from Kaggle, is ideal for data analysis and machine learning, containing comprehensive data on matches, players, and teams across European countries from 2008 to 2016.'}, {'link': 'https://github.com/Abdulraqib20/first-web-scraping/blob/main/laliga.ipynb', 'title': 'La Liga Web Scraping', 'details': \"In this project, I'm scraping data from the La Liga football league website using Python's Beautiful Soup library. The scraped data encompasses player statistics, team rankings, and various other crucial metrics tied to the league.\"}]}, 'status': 'completed', 'expiresAt': '2025-03-09T15:30:42.000Z'}\n"
     ]
    }
   ],
   "source": [
    "scraper = WebsiteScraper()\n",
    "    \n",
    "# Get user input\n",
    "website_url = \"https://raqibcodes.netlify.app/*\"\n",
    "prompt = \"extract the project titles, their details and corresponding links from the projects section\"\n",
    "    \n",
    "# Optional: Add schema fields\n",
    "schema_fields = [\n",
    "    {\"name\": \"Project_title\", \"type\": \"str\"},\n",
    "    {\"name\": \"Details\", \"type\": \"str\"},\n",
    "    {\"name\": \"Project_link\", \"type\": \"str\"}\n",
    "]\n",
    "\n",
    "# Get results\n",
    "result = scraper.scrape_website(website_url, prompt, [])\n",
    "print(\"Results:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'projects': [{'link': 'https://huggingface.co/spaces/raqibcodes/ai_voice_translator',\n",
       "   'title': 'AI Voice Translator',\n",
       "   'details': 'Record your message in English, and receive translations and transcriptions in multiple languages including Russian, French, Korean, Chinese, Spanish, Arabic and Japanese.'},\n",
       "  {'link': 'https://pdfgpt1.streamlit.app/',\n",
       "   'title': 'RAG Chatbot',\n",
       "   'details': 'Developed a Retrieval-Augmented Generation (RAG) chatbot with memory capabilities, supporting diverse document types. Integrated the Chroma vector store for efficient embedding storage and retrieval, optimized responses through advanced prompt engineering and LLM experimentation, and structured a modular, scalable, and collaborative codebase.'},\n",
       "  {'link': 'https://whatsappchats.streamlit.app/',\n",
       "   'title': 'AI-Powered WhatsApp Chat Analysis App',\n",
       "   'details': 'Developed and deployed an AI-powered WhatsApp chat analysis app with a chatbot that processes CSV chat files, enabling users to uncover insights effortlessly. The app integrates advanced NLP algorithms, including emoji analysis and keyword search, to extract meaningful insights from chat data. It features an interactive interface with dynamic visualizations, customizable charts, word clouds, and other plots, enhancing data exploration and user engagement.'},\n",
       "  {'link': 'https://predictsentiment.streamlit.app/',\n",
       "   'title': 'AI-Powered Sentiment Analysis Web App',\n",
       "   'details': 'Developed and deployed an AI-powered Sentiment Analysis web app with integrated Generative AI, allowing users to chat and derive insights from student feedback to enhance learning experiences. The app achieved 96% model accuracy, offering a deeper understanding of student emotions and assisting lecturers in refining teaching strategies. It also features EDA and interactive visualizations, improving data exploration and user engagement.'},\n",
       "  {'link': 'https://booksrecommender.streamlit.app/',\n",
       "   'title': 'Book Recommender System',\n",
       "   'details': \"This web-app is a book recommender system that uses a content-based approach, leveraging book features to recommend similar books to users based on features of the books or characteristics of the items and the user's preferences. Aspects such as plot, characters, and themes that truly engages the user are taken into consideration. Subsequently, book recommendations are provided that align with the user's tastes.\"},\n",
       "  {'link': 'https://github.com/Abdulraqib20/telecom-churn-prediction/tree/main/Notebooks',\n",
       "   'title': 'Telecom Churn Prediction',\n",
       "   'details': \"Addressing a significant industry challenge, this project effectively predicts customer churn in the telecom sector using machine learning. The top-performing model, an XGBoost classifier, achieved an impressive 92% Recall Score in predicting positive cases of 'churn' with other impressive performances in other evaluation metrics used too. Insights from this initiative empower the industry to implement strategic retention measures, mitigating churn and boosting customer satisfaction in a sector grappling with multimillion-dollar losses each month due to customer attrition.\"},\n",
       "  {'link': 'https://github.com/Abdulraqib20/Customer-Personality-Analysis/blob/main/customer_personality_analysis.ipynb',\n",
       "   'title': 'Customer Personality Analysis',\n",
       "   'details': 'This project allows businesses to customize their offerings, products and services to different customer segments, instead of utilizing a generic approach. Through techniques like unsupervised learning, I gain a better understanding of customers, facilitating adjustments to products based on specific needs, behaviors, and concerns of various customer types.'},\n",
       "  {'link': 'https://github.com/Abdulraqib20/Employee-Attrition-Prediction-Using-Gradient-Boosting-Algorithms',\n",
       "   'title': 'Employee Attrition Prediction',\n",
       "   'details': 'This project harnesses the power of data and AI to forecast and analyze employee attribution trends. Gradient Boosting Algorithms including XGBoost, CatBoost, LightGBM and an ensemble of these models were used to develop robust predictive models that provide insights into employee retention, identify patterns, and contribute to strategic talent management.'},\n",
       "  {'link': 'https://github.com/Abdulraqib20/Wrangle-and-Analyze-Data/blob/main/wrangle_act.ipynb',\n",
       "   'title': 'WeRateDogs Twitter Data Analysis',\n",
       "   'details': \"I wrangled, analyzed and visualized the tweet archive of Twitter user @dogrates also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. I wrangled WeRateDogs Twitter data to create interesting and trustworthy analysis and visualizations.\"},\n",
       "  {'link': 'https://github.com/Abdulraqib20/Prosper-Loan-Analysis/blob/main/Part_I_exploration_template.ipynb',\n",
       "   'title': 'Prosper Loan Analysis',\n",
       "   'details': 'In this project, I analyze the Prosper loan dataset using Python to create insightful visualizations. The process involves thorough exploration, data cleaning, and preparation to extract meaningful insights into borrower behavior and loan performance.'},\n",
       "  {'link': 'https://github.com/Abdulraqib20/Soccer-Database-Analysis/blob/main/project.ipynb',\n",
       "   'title': 'Soccer Database Analysis',\n",
       "   'details': 'This project was part of my Data Analyst Nanodegree Programme at Udacity. The soccer database, sourced from Kaggle, is ideal for data analysis and machine learning, containing comprehensive data on matches, players, and teams across European countries from 2008 to 2016.'},\n",
       "  {'link': 'https://github.com/Abdulraqib20/first-web-scraping/blob/main/laliga.ipynb',\n",
       "   'title': 'La Liga Web Scraping',\n",
       "   'details': \"In this project, I'm scraping data from the La Liga football league website using Python's Beautiful Soup library. The scraped data encompasses player statistics, team rankings, and various other crucial metrics tied to the league.\"}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSchema(BaseModel):\n",
    "    mission: str\n",
    "    supports_sso: bool\n",
    "    is_open_source: bool\n",
    "    is_in_yc: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'mission': {'title': 'Mission', 'type': 'string'},\n",
       "  'supports_sso': {'title': 'Supports Sso', 'type': 'boolean'},\n",
       "  'is_open_source': {'title': 'Is Open Source', 'type': 'boolean'},\n",
       "  'is_in_yc': {'title': 'Is In Yc', 'type': 'boolean'}},\n",
       " 'required': ['mission', 'supports_sso', 'is_open_source', 'is_in_yc'],\n",
       " 'title': 'ExtractSchema',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtractSchema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'Project_title': {'title': 'Project Title', 'type': 'string'},\n",
       "  'Details': {'title': 'Details', 'type': 'string'},\n",
       "  'Project_link': {'title': 'Project Link', 'type': 'string'}},\n",
       " 'required': ['Project_title', 'Details', 'Project_link'],\n",
       " 'title': 'ExtractSchema',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.create_schema_from_fields(schema_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "\n",
    "class ExtractSchema(BaseModel):\n",
    "    project_title: str\n",
    "    details: str\n",
    "    project_link: str\n",
    "\n",
    "data = app.extract([\n",
    "  \"https://raqibcodes.netlify.app/*\"], {\n",
    "    'prompt': 'Extract the project titles, their details and corresponding links from the projects section',\n",
    "    'schema': ExtractSchema.model_json_schema(),\n",
    "})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
